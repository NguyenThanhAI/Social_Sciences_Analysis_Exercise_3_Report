\documentclass[14pt, a4paper]{article}
\usepackage{minitoc}
\usepackage[left=3.00cm, right=2.5cm, top=2.00cm, bottom=2.00cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
%\usepackage{algpseudocode}
%\usepackage{algorithm}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{blindtext}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage[utf8]{vietnam}
\usepackage[center]{caption}
\usepackage[shortlabels]{enumitem}
\usepackage{fancyhdr} % header, footer
\usepackage{hyperref} % loại bỏ border với mục lục và công thức
\usepackage[nonumberlist, nopostdot, nogroupskip]{glossaries}
\usepackage{glossary-superragged}
\usepackage{tikz,tkz-tab}
\usepackage{pythonhighlight}
\setglossarystyle{superraggedheaderborder}
\pagestyle{fancy}
%\usepackage[style=numeric,sortcites]{biblatex}
%\addbibresource{ref.bib}
%\usepackage[numbers]{natbib}
\usepackage{indentfirst}
\usepackage[natbib,backend=biber,style=ieee, sorting=ynt]{biblatex}

\usepackage{caption}
\usepackage{subcaption}

\bibliography{ref.bib}

\graphicspath{{./figures/}}

\fancyhf{}
%\rhead{\textbf{Môn học: Các phương pháp thống kê hiện đại trong nghiên cứu Xã hội học}}
\lhead{\textbf{GVHD: TS. Trịnh Quốc Anh}}
\rfoot{\thepage}
\lfoot{\textbf{Học viên thực hiện: Nguyễn Chí Thanh - 21007925}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
%
%\numberwithin{equation}{section}
%\numberwithin{algorithm}{section}
%\numberwithin{figure}{section}
%
%\setlength{\parindent}{0.5cm}
%
%\setcounter{secnumdepth}{3} % Cho phép subsubsection trong report
%\setcounter{tocdepth}{3} % Chèn subsubsection vào bảng mục lục

%\newtheorem{dl}{Định lý}
%\newtheorem{md}{Mệnh đề}
%\newtheorem{bd}{Bổ đề}
%\newtheorem{dn}{Định nghĩa}
%\newtheorem{hq}{Hệ quả}

%\newtheorem{baitap}{Bài tập}
%\newtheorem*{loigiai}{Lời giải}

%\numberwithin{dl}{section}
%\numberwithin{md}{section}
%\numberwithin{bd}{section}
%\numberwithin{dn}{section}
%\numberwithin{hq}{section}

\setlength{\parindent}{0cm}

\newtheorem{dl}{Định lý}
\newtheoremstyle{sltheorem}
{}                % Space above
{}                % Space below
{\normalfont}        % Theorem body font % (default is "\upshape")
{}                % Indent amount
{\bfseries}       % Theorem head font % (default is \mdseries)
{.}               % Punctuation after theorem head % default: no punctuation
{ }               % Space after theorem head
{}                % Theorem head spec
\theoremstyle{sltheorem}
\newtheorem{baitap}{Bài tập}
\newtheoremstyle{soltheorem}
{}                % Space above
{}                % Space below
{\normalfont}        % Theorem body font % (default is "\upshape")
{}                % Indent amount
{\bfseries}       % Theorem head font % (default is \mdseries)
{.}               % Punctuation after theorem head % default: no punctuation
{\newline}               % Space after theorem head
{}                % Theorem head spec
\theoremstyle{soltheorem}
\newtheorem*{loigiai}{Lời giải}

\onehalfspacing

\begin{document}
\begin{titlepage}

    \newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

    \center % Center everything on the page

    %----------------------------------------------------------------------------------------
    %	HEADING SECTIONS
    %----------------------------------------------------------------------------------------
    \textsc{\LARGE Đại học Quốc Gia Hà Nội}\\[0.5cm]
    \textsc{\LARGE Trường đại học Khoa học tự nhiên}\\[0.5cm] % Name of your university/college
    \textsc{\LARGE Khoa Toán - Cơ - Tin học}\\[0.5cm]

    \includegraphics[scale=0.2]{HUS-logo.jpg}\\[0.5cm]

    \textsc{\Large Chuyên ngành: Khoa học dữ liệu}\\[0.5cm] % Major heading such as course name


    %----------------------------------------------------------------------------------------
    %	TITLE SECTION
    %----------------------------------------------------------------------------------------

    \HRule \\[0.4cm]
    { \huge \bfseries Bài tập môn học}\\[0.4cm] % Title of your document
    \HRule \\[1.5cm]

    \textsc{\Large Môn học: Các phương pháp thống kê hiện đại \\ trong nghiên cứu Xã hội học}\\[1cm] % Minor heading such as course title


    \textsc{\Large Bài tập số 3}\\[1cm]


    %----------------------------------------------------------------------------------------
    %	AUTHOR SECTION
    %----------------------------------------------------------------------------------------
    \begin{minipage}{0.4\textwidth}
        \begin{flushleft} \large
        \emph{Giảng viên hướng dẫn:} \\
        TS. Trịnh Quốc Anh % Supervisor's Name
        \end{flushleft}
    \end{minipage}\\[0.5cm]

    \begin{minipage}{0.4\textwidth}
    \begin{flushleft} \large
    \emph{Học viên thực hiện:}\\
    Nguyễn Chí Thanh \\
    MSHV: 21007925 \\ % Your name
    Lớp: Khoa học dữ liệu - K4
    \end{flushleft}
    \end{minipage}


    % If you don't want a supervisor, uncomment the two lines below and remove the section above
    %\Large \emph{Author:}\\
    %John \textsc{Smith}\\[3cm] % Your name

    %----------------------------------------------------------------------------------------
    %	DATE SECTION
    %----------------------------------------------------------------------------------------

    % I don't want day because it is English
    % {\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

    %----------------------------------------------------------------------------------------
    %	LOGO SECTION
    %----------------------------------------------------------------------------------------

    %\includegraphics{logo/rsz_3logo-khtn.png}\\[1cm] % Include a department/university logo - this will require the graphicx package

    %----------------------------------------------------------------------------------------

    \vfill % Fill the rest of the page with whitespace

\end{titlepage}

\nocite{*}

\newpage

\begin{baitap}
    Áp dụng các phương pháp Ridge, LASSO, và PCA cho dữ liệu Sonar.

    Hãy so sánh các kết quả thu được và biện luận để đưa ra mô hình phù hợp nhất.
\end{baitap}

\begin{loigiai}
    Hàm mất mát của mô hình logistic regression:
    \begin{equation*}
        \mathcal{L} = -\dfrac{1}{N}\sum_{i=1}^N (y_i \log \hat{y}_i + (1-y_i)\log(1-\hat{y}_i)) 
    \end{equation*}

    với $\hat{y}=p(y=1\vert \bold{x})=f(\bold{x}) = \sigma(w_0 + w_1 x_1 + w_2 x_2 + \dots w_n x_n) = \sigma(\bold{w}^T \bold{x})$ là kết quả đầu ra của mô hình logistic tương ứng với đầu vào $\bold{x}$.

    Hàm mất mát của phương pháp Ridge:

    \begin{equation*}
        \mathcal{L}_{\mathrm{Ridge}} = -\dfrac{1}{N}\sum_{i=1}^N (y_i \log \hat{y}_i + (1-y_i)\log(1-\hat{y}_i)) + \lambda \lVert \bold{w} \rVert_2^2
    \end{equation*}

    Hàm mất mát của phương pháp LASSO:

    \begin{equation*}
        \mathcal{L}_{\mathrm{Ridge}} = -\dfrac{1}{N}\sum_{i=1}^N (y_i \log \hat{y}_i + (1-y_i)\log(1-\hat{y}_i)) + \lambda \lVert \bold{w} \rVert_1
    \end{equation*}

    Ta sử dụng 5 mô hình đã được sử dụng trong bài tập trước:

    \begin{itemize}
        \item Mô hình gồm tất cả các biến (ký hiệu là full)
        \item Mô hình forward được chọn ở bài tập trước (ký hiệu là m.f)
        \item Mô hình backward được chọn ở bài tập trước (ký hiệu là m.b)
        \item Mô hình forward-backward được chọn ở bài tập trước (ký hiệu là m.bo)
        \item Mô hình các biến được trích xuất từ thuật toán PCA giữ lại 95 \% thông tin (ký hiệu là pca\_x với x là thuật toán PCA giữ lại x \% thông tin)
    \end{itemize}

    Để chọn ra mô hình tốt nhất, mỗi mô hình đi kèm với cách chỉnh định sẽ được tối ưu hóa các siêu tham số gồm:

    \begin{itemize}
        \item Phương pháp tiền xử lý: min-max scaling, standardize.
        \item Tốc độ học được biến đổi trong quá trình huấn luyện theo các cách: fixed, cyclic, inverse\_decay, exponential\_decay, factor\_decay, squareroot, cosine.
        \item Các tốc độ học khác nhau: 1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 5, 10
        \item Các thuật toán tối ưu hóa, 12 thuật toán tối ưu hóa được sử dụng gồm:
        \begin{itemize}
            \item Gradient Descent
            \item Adam
            \item Avagrad
            \item RAdam
            \item Momentum
            \item Adagrad
            \item RMSProp
            \item Adadelta
            \item Adamax
            \item Nadam
            \item AMSGrad
            \item AdaBelief
        \end{itemize}

        Sơ đồ mã giả của các thuật toán tối ưu hóa được trình bày ở phụ lục \ref{Optimization-Algorithms} và chi tiết các phương pháp điều chỉnh tốc độ học được trình bày ở phục lục \ref{LRScheduler}
    \end{itemize}

    Các siêu tham số trên sẽ được huấn luyện sử dụng code python và numpy không sử dụng thư viện khác.
    Việc sử dụng code python và numpy để có thể thử với các thuật toán tối ưu hóa khác mà thư viện sklearn không có.
    Hệ số $\lambda$ trong quá trình huấn luyện các mô hình được tự cài đặt bằng python và numpy được cố định là 1e-3 (do hạn chế về mặt thời gian).
    Kết quả của quá trình tối ưu hóa siêu tham số cho các mô hình được cài đặt bằng python và numpy được trình bày ở bảng \ref{tab:custom-hyperparameter-result} với cột bên trái ngoài cùng là loại mô hình (được hình thành từ các biến giải thích sử dụng và phương pháp chỉnh định) các cột bên phải là tổ hợp các siêu tham số tối ưu tương ứng.

    Ngoài ra ta vẫn huấn luyện một tập các mô hình được huấn luyện sử dụng thư viện sklearn.
    Vì vấn đề thời gian đối với các mô hình được huấn luyện trên sklearn chỉ gồm 3 mô hình:
    \begin{itemize}
        \item Mô hình gồm tất cả các biến
        \item Mô hình forward-backward
        \item Mô hình các biến được trích xuất từ thuật toán PCA giữ lại 95 \% thông tin
    \end{itemize}
    Siêu tham số của các mô hình được huấn luyện sử dụng sklearn bao gồm:

    \begin{itemize}
        \item Phương pháp tiền xử lý: min-max scaling, standardize.
        \item Các hệ số $C$ là nghịch đảo của $\lambda$
        \item Các thuật toán tối ưu hóa, 6 thuật toán tối ưu hóa được sử dụng gồm:
        \begin{itemize}
            \item lbfgs
            \item liblinear
            \item newton-cg 
            \item newton-cholesky
            \item sag 
            \item saga
        \end{itemize}
    \end{itemize}

    Phương pháp chỉnh định "l1" được gọi là Lasso, phương pháp chỉnh định "l2" được gọi là Ridge, không chỉnh định ta gọi là Naive
    Mỗi một cặp thông tin gồm phương pháp chỉnh định ("l1", "l2", không chỉnh định) và mô hình sẽ được chọn ra bộ siêu tham số tốt nhất từ các siêu tham số được đề cập ở trên bằng độ đo AUC-ROC trên tập test.
    Ta sẽ cố định tập test ngay từ đầu.
    Như vậy ta sẽ có 5 (mô hình) $\times$ 3 ("l1"-lasso, "l2"-ridge, không chỉnh định) = 15 mô hình ứng viên được cài đặt bằng ngôn ngữ python và numpy được chọn từ quá trình tối ưu hóa siêu tham số và 9 mô hình ứng viên được huấn luyện bằng thư viện sklearn.
    Như vậy ta có 24 mô hình ứng viên.

    Từ các mô hình ứng viên, để chọn ra mô hình tốt nhất từ nhiều yếu tố như $AUC-ROC, AIC, BIC$ cũng như từ trung bình hình học G-Means giữa độ nhạy (Sensitivity) và độ đặc hiệu (Specificity) ứng với ngưỡng xác suất $p^*$ ($p^*$ làm cho G-Means cực đại) được chọn.

    \begin{table}[h!]
        \centering
        \begin{tabular}{|c | c | c | c | c |}
            \hline
            Mô hình & Phương pháp chuẩn hóa & Thuật toán tối ưu & Scheduler & Init learning rate \\
            \hline
            \hline
            full - Naive & standardize & RMSProp & inverse decay & 5 \\
            \hline
            m.bo - Naive & standardize & AdaBelief & fixed & 5 \\
            \hline
            m.f - Naive & standardize & RMSProp & squareroot &5 \\
            \hline
            m.b - Naive & standardize & RMSProp & cosine & 1 \\
            \hline
            pca\_95 - Naive & standardize & Gradient\_Descent & inverse\_decay & 10 \\
            \hline
            full - Lasso & minmax & Avagrad & squareroot & 10 \\
            \hline
            m.bo - Lasso & standardize & Adam & exponential\_decay & 10 \\
            \hline
            m.f - Lasso & standardize & RMSProp & fixed &2 \\
            \hline
            m.b - Lasso & standardize & AMSGrad & cyclic & 5 \\
            \hline
            pca\_95 - Lasso & standardize & Gradient\_Descent & cosine & 0.01 \\
            \hline
            full - Ridge & standardize & RAdam & fixed & 10 \\
            \hline
            m.bo - Ridge & standardize & Adam & fixed & 5 \\
            \hline
            m.f - Ridge & standardize & RAdam & fixed & 10 \\
            \hline
            m.b - Ridge & standardize & RAdam & fixed & 10 \\
            \hline
            pca\_95 - Ridge & standardize & RAdam & fixed & 10 \\
            \hline
        \end{tabular}
        \caption{Các siêu tham số tốt nhất với từng mô hình (mỗi mô hình được xác định bằng các biến \\ giải thích và phương pháp chỉnh định (naive, ridge, lasso))}
        \label{tab:custom-hyperparameter-result}
    \end{table}


    \begin{table}[h!]
        \centering
        \begin{tabular}{|c | c | c | c | c |}
            \hline
            Mô hình & Random\_state & Phương pháp chuẩn hóa & Thuật toán tối ưu & C \\
            \hline
            \hline
            Sklearn full - Naive & 379 & minmax & sag & 0.0001 \\
            \hline
            Sklearn m.bo - Naive & 953 & standardize & sag & 0.0001 \\
            \hline
            Sklearn pca\_95 - Naive & 431 & minmax & sag & 0.0001 \\
            \hline
            Sklearn full - Lasso & 587 & standardize & saga & 1000 \\
            \hline
            Sklearn m.bo - Lasso & 0 & standardize & liblinear & 10 \\
            \hline
            Sklearn pca\_95 - Lasso & 2 & minmax & saga & 0.1 \\
            \hline
            Sklearn full - Ridge & 0 & minmax & lbfgs & 10 \\
            \hline
            Sklearn m.bo - Ridge & 85 & standardize & sag & 100 \\
            \hline
            Sklearn pca\_95 - Ridge & 0 & minmax & lbfgs & 10 \\
            \hline
        \end{tabular}
        \caption{Các siêu tham số tốt nhất với từng mô hình được huấn luyện sử dụng thư viện sklearn (mỗi mô hình được xác định bằng các biến giải thích và phương pháp chỉnh định (naive, ridge, lasso))}
        \label{tab:sklearn-hyperparameter-result}
    \end{table}

    Random\_state được chính là seed để khởi tạo các tham số của mô hình.
    Còn tập dữ liệu được chia tập train và tập test một cách cố định từ ban đầu (sử dụng random\_state 42)

    Kết quả của quá trình tối ưu hóa siêu tham số cho các mô hình được cài đặt bằng python và numpy được trình bày ở bảng \ref{tab:sklearn-hyperparameter-result}


    Để chọn ra ngưỡng xác suất $p^*$ tốt nhất với từng mô hình ta tìm ngưỡng xác suất sao cho cực đại hóa trung bình hình học giữa độ nhạy (Sensitivity) và độ đặc hiệu (Specificity).
    Ta có công thức tính các đại lượng trên:

    \begin{equation*}
        \begin{aligned}
            \mathrm{TPR} = \mathrm{Precision} = \mathrm{Sensitivity} &= \dfrac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}} \\
            \mathrm{FPR} &= \dfrac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}} \\
            \mathrm{Specificity} = 1 - \mathrm{FPR} &= \dfrac{\mathrm{TN}}{\mathrm{FP} + \mathrm{TN}}
        \end{aligned}
    \end{equation*}

    Trung bình hình học giữa giữa độ nhạy (Sensitivity) và độ đặc hiệu (Specificity) là:
    \begin{equation*}
        \mathrm{G-Means} = \sqrt{\mathrm{Sensitivity} \times \mathrm{Specificity}} = \sqrt{\dfrac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}} \times \dfrac{\mathrm{TN}}{\mathrm{FP} + \mathrm{TN}}}
    \end{equation*}

    Như ta đã biết đường cong ROC được xây dựng bằng cách chọn các ngưỡng xác suất $p$ để một mẫu được phân loại thành nhãn 1 hay nhãn 0 (mẫu được xem là nhãn 1 nếukết quả đầu ra lớn hơn 1 và ngược lại được xem là nhãn 0).
    Như vậy với mỗi ngưỡng xác suất sẽ có một bảng ma trận nhầm lẫn khác nhau (TP, TN, FP, FN thay đổi dựa theo ngưỡng xác suất) hay sẽ có các giá trị độ nhạy (Sensitivity) và độ đặc hiệu (Specificity) khác nhau từ đó dẫn đến trung bình hình học G-Means giữa hai đại lượng này cũng thay đổi.
    Ta chọn $p^*$ sao cho làm cực đại hóa giá trị G-Means:

    \begin{equation*}
        p^* = \underset{p}{\mathrm{argmax}} \mathrm{G-Means} = \underset{p}{\mathrm{argmax}} \sqrt{\mathrm{Sensitivity} \times \mathrm{Specificity}} = \underset{p}{\mathrm{argmax}} \sqrt{\dfrac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}} \times \dfrac{\mathrm{TN}}{\mathrm{FP} + \mathrm{TN}}}
    \end{equation*}

    Để tính giá trị $p^*$ ta sử dụng đoạn code sau:

    \begin{python}
y_pred = model.predict(x=test_x, return_prob=True)
fpr, tpr, thresholds = roc_curve(y_true=test_y, y_score=y_pred)
area_roc = auc(fpr, tpr)
gmeans = compute_gmeans(tpr=tpr, fpr=fpr)
        
idx = np.argmax(gmeans)
prob_star = thresholds[idx]
gmean_star = gmeans[idx]
    \end{python}
\end{loigiai}

\newpage
\printbibliography[title={TÀI LIỆU THAM KHẢO}]

\newpage
\appendix

\section{Các thuật toán tối ưu hóa} \label{Optimization-Algorithms}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}

    \For{$t \gets 1$ \KwSty{to} $T$} {
        $g_t \gets \nabla \ell(w_{t-1})$\;
        $w_t \gets w_{t-1} - \alpha_t g_t$
    }
    \Return{$w_T$}\;
    \caption{Thuật toán Gradient Descent}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, hệ số $\beta_1$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}
    $m_0 \gets 0$\;
    \For{$t \gets 1$ \KwSty{to} $T$} {
        $g_t \gets \nabla \ell(w_{t-1})$\;
        $m_t \gets \beta_1 m_{t-1} + (1-\beta_1) g_t$\;
        $w_t \gets w_{t-1} - \alpha_t m_t$\;
    }
    \Return{$w_T$}\;
    \caption{Thuật toán Momentum}
\end{algorithm}


\begin{algorithm}[h!]
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}

    $v_0 \gets 0$\;
    \For{$t \gets 1$ \KwSty{to} $T$} {
        $g_t \gets \nabla \ell(w_{t-1})$\;
        $v_t \gets v_{t-1} + g_t^2$\;
        $w_t \gets w_{t-1} - \alpha_t \dfrac{g_t}{\sqrt{v_t} + \epsilon}$\;
    }

    \Return{$w_T$}\;
    \caption{Thuật toán Adagrad}
\end{algorithm}

\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, hệ số $\beta$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}

    $v_0 \gets 0$\;
    $d_0 \gets 0$\;

    \For{$t \gets 1$ \KwSty{to} $T$} {
        $g_t \gets \nabla \ell (w_{t-1})$\;
        $v_t \gets \beta v_{t-1} + (1-\beta) g_t^2$\;
        $\Delta w \gets -\alpha_t \dfrac{\sqrt{d_{t-1} + \epsilon}g_t}{\sqrt{v_t + \epsilon}}$\;
        $w_t \gets w_{t-1} + \delta w$\;
        $d_t \gets \beta d_{t-1} + (1-\beta) \delta w^2$\;
    }

    \Return{$w_T$}\;
    \caption{Thuật toán Adadelta}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, hệ số $\beta$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}

    $v_0 \gets 0$\;
    \For{$t \gets 1$ \KwSty{to} $T$} {
        $g_t \gets \nabla \ell (w_{t-1})$\;
        $v_t \gets \beta v_{t-1} + (1-\beta) g_t^2$\;
        $w_t \gets w_{t-1} - \alpha_t \dfrac{g_t}{\sqrt{v_t} + \epsilon}$\;
    }

    \Return{$w_T$}\;
    \caption{Thuật toán RMSProp}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, các hệ số $\beta_1, \beta_2$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}

    $m_0 \gets 0$\;
    $v_0 \gets 0$\;

    \For{$t \gets 1$ \KwSty{to} $T$} {
        $g_t \gets \nabla \ell(w_{t-1})$\;
        $m_t \gets \beta_1 m_{t-1} + (1 - \beta_1)g_t$\;
        $v_t \gets \beta_2 v_{t-1} + (1 - \beta_2)g_t^2$\;
        $\hat{m}_t \gets \dfrac{m_t}{1 - \beta_1^t}$\;
        $\hat{v}_t \gets \dfrac{v_t}{1 - \beta_2^t}$\;
        $w_t \gets w_{t-1} - \alpha_t \dfrac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$\;
    }

    \Return{$w_T$}\;
    \caption{Thuật toán Adam}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, các hệ số $\beta_1, \beta_2$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}

    $m_0 \gets 0$\;
    $u_0 \gets 0$\;

    \For{$t \gets 1$ \KwSty{to} $T$} {
        $g_t \gets \nabla \ell(w_{t-1})$\;
        $m_t \gets \beta_1 m_{t-1} + (1 - \beta_1)g_t$\;
        $u_t \gets \max (\beta u_{t-1}, \lVert g_t \rVert_{\infty})$\;
        $w_t \gets w_{t-1} - \alpha_t \dfrac{m_t}{(1-\beta_1^t)u_t + \epsilon}$\;
    }

    \Return{$w_T$}\;
    \caption{Thuật toán Adamax}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, các hệ số $\beta_1, \beta_2$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}

    $m_0 \gets 0$\;
    $v_0 \gets 0$\;

    \For{$t \gets 1$ \KwSty{to} $T$} {
        $g_t \gets \nabla \ell(w_{t-1})$\;
        $m_t \gets \beta_1 m_{t-1} + (1 - \beta_1)g_t$\;
        $v_t \gets \beta_2 v_{t-1} + (1 - \beta_2)g_t^2$\;
        $\hat{v}_t \gets \max(\hat{v}_{t-1}, v_t)$\;
        $w_t \gets w_{t-1} - \alpha_t \dfrac{m_t}{\sqrt{\hat{v}_t} + \epsilon}$\;
    }

    \Return{$w_T$}\;
    \caption{Thuật toán AMSGrad}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, các hệ số $\beta_1, \beta_2$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}

    $m_0 \gets 0$\;
    $v_0 \gets 0$\;

    \For{$t \gets 1$ \KwSty{to} $T$} {
        $g_t \gets \nabla \ell(w_{t-1})$\;
        $m_t \gets \beta_1 m_{t-1} + (1 - \beta_1)g_t$\;
        $v_t \gets \beta_2 v_{t-1} + (1 - \beta_2)g_t^2$\;
        $\hat{m}_t \gets \dfrac{m_t}{1 - \beta_1^t}$\;
        $\hat{v}_t \gets \dfrac{v_t}{1 - \beta_2^t}$\;
        $w_t \gets w_{t-1} - \dfrac{\alpha_t}{\sqrt{\hat{v}_t} + \epsilon} \Bigg( \beta_1 \hat{m}_t + \dfrac{1 - \beta_1}{1 - \beta_1^t}g_t \Bigg)$\;
    }

    \Return{$w_T$}\;
    \caption{Thuật toán Nadam}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, các hệ số $\beta_1, \beta_2$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}

    $m_0 \gets 0$\;
    $s_0 \gets 0$\;

    \For{$t \gets 1$ \KwSty{to} $T$} {
        $g_t \gets \nabla \ell(w_{t-1})$\;
        $m_t \gets \beta_1 m_{t-1} + (1 - \beta_1)g_t$\;
        $s_t \gets \beta_2 s_{t-1} + (1-\beta_2)(g_t - m_t)^2 + \epsilon$\;
        $\hat{m}_t \gets \dfrac{m_t}{1 - \beta_1^t}$\;
        $\hat{s}_t \gets \dfrac{s_t}{1 - \beta_2^2}$\;
        $w_t \gets w_{t-1} - \alpha_t \dfrac{\hat{m}_t}{\sqrt{\hat{s}_t} + \epsilon}$
    }

    \Return{$w_T$}\;
    \caption{Thuật toán AdaBelief}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, các hệ số $\beta_1, \beta_2$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}
    $m_0 \gets 0$\;
    $v_0 \gets 0$\;
    $\rho_{\infty} \gets 2/(1-\beta_2)-1$\;
    \For{$t \gets 1$ \KwSty{to} $T$}{
        $g_t \gets \nabla \ell(w_{t-1})$\;
        $m_t \gets \beta_1 m_{t-1} + (1 - \beta_1)g_t$\;
        $v_t \gets \beta_2 v_{t-1} + (1 - \beta_2)g_t^2$\;
        $\widehat{m}_t \gets m_t / (1 - \beta_1^t)$\;
        $\rho_t \gets \rho_{\infty} - 2t\beta_2^t / (1 - \beta_2^t)$\;
        \If {$\rho_t < 4$} {
            $\widehat{v}_t \gets \sqrt{v_t / (1 - \beta_2^t)}$\;
            $r_t \gets \sqrt{\frac{(\rho_t - 4)(\rho_t - 2)\rho_{\infty}}{(\rho_{\infty} - 4)(\rho_{\infty} - 2)\rho_t}}$\;
            $w_t \gets w_{t-1} - \alpha_t r_t \widehat{m}_t / (\widehat{v}_t + \epsilon)$\;
        } \Else {
            $w_t \gets w_{t-1} - \alpha_t \widehat{m}_t$\;
        }
    }
    \Return{$w_T$}\;
    \caption{Thuật toán RAdam}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, hệ số $\beta_1$, $w_0$ khởi tạo, hàm mục tiêu $\ell(w)$}
    \KwOut{$w$ đã được học}
    $m_0 \gets 0$\;
    $v_0 \gets 0$\;

    \For{$t \gets 1$ \KwSty{to} $T$}{
        $g_t \gets \nabla \ell(w_{t-1})$\;
        $m_t \gets \beta_1 m_{t-1} + (1 - \beta_1)g_t$\;
        $\eta_t \gets \dfrac{1}{\sqrt{v_{t-1}} + \epsilon}$\;
        $w_t \gets w_{t-1} - \alpha_t \dfrac{\eta_t}{\lVert \eta_t / \sqrt{d} \rVert_2 \odot m_t}$\;
        $v_t \gets \beta_2 v_{t-1} + (1-\beta_2)g_t^2$\;
    }
    \Return{$w_T$}\;
    \caption{Thuật toán Avagrad}
\end{algorithm}

\section{Các phương pháp điều chỉnh tốc độ học} \label{LRScheduler}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{Tốc độ học ban đầu $\alpha$}
    \For{$t \gets 0$ \KwSty{to} $T$} {
        $t \gets t + 1$\;
        $\alpha_t \gets \alpha$\;
    }
    \caption{Điều chỉnh tốc độ học theo phương pháp cố định}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{Tốc độ học ban đầu $\alpha$}
    \For{$t \gets 0$ \KwSty{to} $T$} {
        $t \gets t + 1$\;
        $\alpha_t \gets \dfrac{\alpha}{t}$\;
    }
    \caption{Điều chỉnh tốc độ học theo phương pháp inverse decay}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{min\_lr, max\_lr, num\_increase, num\_decrease}
    \For{$t \gets 0$ \KwSty{to} $T$} {
        $t \gets t + 1$\;
        res\_step $\gets$ num\_steps mod (num\_increase + num\_decrease)\;
        \If{res\_step < num\_increase} {
            $\alpha_t \gets$  min\_lr + (max\_lr - min\_lr) $\times$ res\_step / num\_increase\;
        } \Else {
            $\alpha_t \gets$ max\_lr - (max\_lr - min\_lr) $\times$ (res\_step - num\_increase) / (num\_decrease)\;
        }
    }
    \caption{Điều chỉnh tốc độ học theo phương pháp cyclic}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{min\_lr, max\_lr, max\_steps}
    \For{$t \gets 0$ \KwSty{to} $T$} {
        $t \gets t + 1$\;
        \If{$t \geq$ max\_steps} {
            $\alpha_t \gets$ min\_lr\;
        } \Else {
            $\alpha_t \gets$ max\_lr - num\_steps $\times$ (init\_lr - min\_lr) / max\_steps\;
        }
    }
    \caption{Điều chỉnh tốc độ học theo phương pháp giảm tuyến tính}
\end{algorithm}

\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{Tốc độ học ban đầu $\alpha$, tốc độ giảm $k$}
    \For{$t \gets 0$ \KwSty{to} $T$} {
        $t \gets t + 1$\;
        $\alpha_t \gets \alpha \exp(-k \times t)$\;
    }
    \caption{Điều chỉnh tốc độ học theo phương pháp giảm theo hàm mũ}
\end{algorithm}

\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{Tốc độ học ban đầu $\alpha$, hệ số giảm $k < 1$, chu kỳ giảm $T$}
    \For{$t \gets 0$ \KwSty{to} $T$} {
        $t \gets t + 1$\;
        \If {$t \% T = 0$} {
            $\alpha_t \gets \alpha \times k$\;
        } \Else {
            $\alpha_t \gets \alpha_{t-1}$\;
        }
    }
    \caption{Điều chỉnh tốc độ học theo phương pháp giảm theo hàm mũ}
\end{algorithm}

\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{Tốc độ học ban đầu $\alpha$}
    \For{$t \gets 0$ \KwSty{to} $T$} {
        $t \gets t + 1$\;
        $\alpha_t \gets \dfrac{\alpha}{\sqrt{t}}$\;
    }
    \caption{Điều chỉnh tốc độ học theo phương pháp giảm theo hàm căn}
\end{algorithm}


\begin{algorithm}[h!]
    \DontPrintSemicolon
    \KwIn{min\_lr, max\_lr, max\_steps}
    \For{$t \gets 0$ \KwSty{to} $T$} {
        $t \gets t + 1$\;
        $\alpha_t \gets$ min\_lr + $\dfrac{1}{2}$ (max\_lr - min\_lr) (1 + $\cos$((num\_steps $\times \pi$)/max\_steps))\;
    }
    \caption{Điều chỉnh tốc độ học theo phương pháp cosine}
\end{algorithm}


\end{document}